{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sergey/.conda/envs/s2and/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "module compiled against API version 0xe but this version of numpy is 0xd",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: module compiled against API version 0xe but this version of numpy is 0xd"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-09 02:34:21,662 - s2and - WARNING - You haven't set `main_data_dir` in data/path_config.json! Using data/ as default data directory.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pickle\n",
    "from os.path import join\n",
    "from random import sample\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from s2and.consts import CONFIG\n",
    "from s2and.data import PDData\n",
    "from s2and.featurizer import featurize, FeaturizationInfo, many_pairs_featurize\n",
    "from s2and.model import PairwiseModeler, Clusterer, FastCluster\n",
    "from s2and.eval import pairwise_eval, cluster_eval, facet_eval\n",
    "from s2and.consts import FEATURIZER_VERSION, DEFAULT_CHUNK_SIZE, PROJECT_ROOT_PATH\n",
    "from s2and.file_cache import cached_path\n",
    "from sklearn.model_selection import GroupKFold, train_test_split\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(context='talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "with open('/net/nfs2.s2-research/shaurya/projects/s2pac/final_formatting/data/papers.v2.json', 'r') as f:\n",
    "    papers = json.load(f)\n",
    "\n",
    "with open('/net/nfs2.s2-research/shaurya/projects/s2pac/final_formatting/data/clusters.v2.json', 'r') as f:\n",
    "    clusters = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('43808411', {'authors': [{'position': 0, 'first': 'Ursula', 'middle': [], 'last': 'Hofer', 'suffix': None, 'affiliations': [], 'email': None}], 'abstract': None, 'references': [], 'paper_id': 43808411, 'source': 'Medline', 'doi': '10.1038/nrmicro3092', 'pmid': 23872943, 'title': 'Viral evolution: Variation in the gut virome.', 'year': 2013, 'venue': 'Nature reviews. Microbiology', 'publicationtypes': ['LettersAndComments', 'JournalArticle'], 'fieldsofstudy': ['Medicine'], 'journal_name': '\\n          596\\n        ', 'block': 'viralevolution', 'corpus_paper_id': 19963}) ('thefboxproteinslmbrestrictstheactiv_6551263', {'cluster_id': 'thefboxproteinslmbrestrictstheactiv_6551263', 'paper_ids': [3673658920, 3503591660, 2051795599], 'model_version': -1})\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(papers.items())), next(iter(clusters.items())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{str}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([type(key) for key in papers.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190\n"
     ]
    }
   ],
   "source": [
    "# step 1: we need to make sure that every paper is in a cluster that is inside the *correct* block\n",
    "# if not, we delete the paper from the cluster in the incorrect block\n",
    "blocks = {str(k): v['block'] for k, v  in papers.items()}\n",
    "blocks.update({int(k): v['block'] for k, v  in papers.items()})\n",
    "\n",
    "times_trimmed = 0\n",
    "for cluster_id in clusters.keys():\n",
    "    papers_loop = clusters[cluster_id]['paper_ids']\n",
    "    this_block = cluster_id.split('_')[0]\n",
    "    papers_for_this_block = [p for p in papers_loop if blocks[p] == this_block]\n",
    "    clusters[cluster_id]['paper_ids'] = papers_for_this_block\n",
    "    if len(papers_for_this_block) < len(papers_loop):\n",
    "        times_trimmed += 1\n",
    "print(times_trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: a bunch of papers are in clusters AND in orphans. we need to remove them from the orphans\n",
    "from collections import defaultdict\n",
    "\n",
    "papers_to_clusters = defaultdict(set)\n",
    "for cluster_id, cluster in clusters.items():\n",
    "    for paper_id in cluster['paper_ids']:\n",
    "        papers_to_clusters[paper_id].add(cluster_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11092"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_removed = 0\n",
    "for paper_id, cluster_ids in papers_to_clusters.items():\n",
    "    if len(cluster_ids) > 1:\n",
    "        orphan_clusters = [c for c in cluster_ids if c.endswith('_orphans')]\n",
    "        orphan_cluster_blocks = [c.split('_')[0] for c in orphan_clusters]\n",
    "        nonorphan_clusters = [c for c in cluster_ids if not c.endswith('_orphans')]\n",
    "        nonorphan_clusters_blocks = [c.split('_')[0] for c in nonorphan_clusters]\n",
    "        # if the paper is in an orphan cluster, and there is a non-orphan cluster in the same block, remove it from the orphan cluster\n",
    "        to_remove = [c for b, c in zip(orphan_cluster_blocks, orphan_clusters) if b in nonorphan_clusters_blocks]\n",
    "        total_removed += len(to_remove)\n",
    "        for cluster_id in to_remove:\n",
    "            try: # i really don't know the types of these keys\n",
    "                clusters[cluster_id]['paper_ids'].remove(str(paper_id))\n",
    "            except:\n",
    "                clusters[cluster_id]['paper_ids'].remove(int(paper_id))\n",
    "total_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: a bunch of papers are in multiple clusters in the SAME block. probably easiest to just remove these blocks entirely\n",
    "papers_to_clusters = defaultdict(set)\n",
    "for cluster_id, cluster in clusters.items():\n",
    "    for paper_id in cluster['paper_ids']:\n",
    "        papers_to_clusters[paper_id].add(cluster_id)\n",
    "        \n",
    "papers_in_multiple_clusters = set([int(p) for p, c in papers_to_clusters.items() if len(c) > 1] + [str(p) for p, c in papers_to_clusters.items() if len(c) > 1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_id in clusters.keys():\n",
    "    paper_ids = clusters[cluster_id]['paper_ids']\n",
    "    paper_ids_trimmed = [p for p in paper_ids if p not in papers_in_multiple_clusters]\n",
    "    clusters[cluster_id]['paper_ids'] = paper_ids_trimmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only papers that are in a cluster\n",
    "papers_in_clusters = set()\n",
    "for cluster_id in clusters.keys():\n",
    "    papers_in_clusters.update(set([str(i) for i in clusters[cluster_id]['paper_ids']]))\n",
    "\n",
    "papers = {k: v for k, v in papers.items() if str(k) in papers_in_clusters}\n",
    "assert papers_in_clusters == set(papers.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save next version\n",
    "with open('/net/nfs2.s2-research/shaurya/projects/s2pac/final_formatting/data/papers.v3.json', 'w') as f:\n",
    "    json.dump(papers, f)\n",
    "\n",
    "with open('/net/nfs2.s2-research/shaurya/projects/s2pac/final_formatting/data/clusters.v3.json', 'w') as f:\n",
    "    json.dump(clusters, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now downsample the blocks that have 100% accuracy\n",
    "# load the data\n",
    "with open('/net/nfs2.s2-research/shaurya/projects/s2pac/final_formatting/data/papers.v3.json', 'r') as f:\n",
    "    papers = json.load(f)\n",
    "\n",
    "with open('/net/nfs2.s2-research/shaurya/projects/s2pac/final_formatting/data/clusters.v3.json', 'r') as f:\n",
    "    clusters = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's drop a subset of the \"easy\" blocks\n",
    "df = pd.read_csv(join(PROJECT_ROOT_PATH, \"data\", \"block_removal_candidates.csv\"))\n",
    "easy_blocks = df['block'][df.accuracy == 1.0].values\n",
    "not_easy_blocks = df['block'][df.accuracy != 1.0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34204\n",
      "47147\n",
      "47334\n",
      "47121\n"
     ]
    }
   ],
   "source": [
    "# choose a random half of the easy blocks and keep all the hard blocks\n",
    "keep_blocks = set(np.random.choice(easy_blocks, size=len(easy_blocks)//2, replace=False))\n",
    "print(len(keep_blocks))\n",
    "keep_blocks.update(not_easy_blocks)\n",
    "print(len(keep_blocks))\n",
    "keep_blocks.update(df['block'][df.keep_count > 0].values)\n",
    "print(len(keep_blocks))\n",
    "keep_blocks -= set(df['block'][df.remove_count > 0].values)\n",
    "print(len(keep_blocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the papers in the blocks we want to keep\n",
    "papers_sub = {k: v for k, v in papers.items() if v['block'] in keep_blocks}\n",
    "\n",
    "# keep only clusters where all of the papers are papers2\n",
    "clusters_sub = {k: v for k, v in clusters.items() if all([str(p) in papers_sub for p in v['paper_ids']])}\n",
    "\n",
    "# keep only papers that are in a cluster\n",
    "papers_in_clusters = set()\n",
    "for cluster_id in clusters_sub.keys():\n",
    "    papers_in_clusters.update(set([str(i) for i in clusters[cluster_id]['paper_ids']]))\n",
    "    \n",
    "assert papers_in_clusters == set(papers_sub.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save next version\n",
    "with open('/net/nfs2.s2-research/shaurya/projects/s2pac/final_formatting/data/papers.v3_hard.json', 'w') as f:\n",
    "    json.dump(papers_sub, f)\n",
    "\n",
    "with open('/net/nfs2.s2-research/shaurya/projects/s2pac/final_formatting/data/clusters.v3_hard.json', 'w') as f:\n",
    "    json.dump(clusters_sub, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('s2and')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a324344a38a2b71ef39af09a7448bcf45e5c27cf7a633e909c54afe4cf2573d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
