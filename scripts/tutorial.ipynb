{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Welcome to the S2AND tutorial!\n",
    "\n",
    "We will cover a few aspects of the S2AND pipeline:\n",
    "\n",
    "(1) Load a test dataset.\n",
    "(2) Fit a pairwise model + bells & whistles.\n",
    "(3) Fit a clusterer.\n",
    "(4) Evaluate the pairwise model and the clusterer.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "import json\n",
    "import copy\n",
    "import argparse\n",
    "import logging\n",
    "import pickle\n",
    "from typing import Dict, Any, Optional, List\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from s2and.data import PDData\n",
    "from s2and.featurizer import featurize, FeaturizationInfo\n",
    "from s2and.model import PairwiseModeler, Clusterer, FastCluster\n",
    "from s2and.eval import pairwise_eval, cluster_eval, facet_eval\n",
    "from s2and.consts import FEATURIZER_VERSION, DEFAULT_CHUNK_SIZE, PROJECT_ROOT_PATH\n",
    "from s2and.file_cache import cached_path\n",
    "from s2and.plotting_utils import plot_facets\n",
    "from hyperopt import hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the random seed we used for the ablations table\n",
    "random_seed = 42\n",
    "# number of cpus to use\n",
    "n_jobs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-10 19:33:34,881 - s2and - INFO - loading papers\n",
      "2022-10-10 19:33:35,115 - s2and - INFO - loaded papers\n",
      "2022-10-10 19:33:35,116 - s2and - INFO - loading clusters\n",
      "2022-10-10 19:33:35,117 - s2and - INFO - loaded clusters, loading specter\n",
      "2022-10-10 19:33:35,118 - s2and - INFO - loaded specter, loading cluster seeds\n",
      "2022-10-10 19:33:35,118 - s2and - INFO - loaded cluster seeds\n",
      "2022-10-10 19:33:35,119 - s2and - INFO - making paper to cluster id\n",
      "2022-10-10 19:33:35,121 - s2and - INFO - made paper to cluster id\n",
      "2022-10-10 19:33:35,128 - s2and - INFO - preprocessing papers\n",
      "Preprocessing papers: 100%|██████████| 3601/3601 [00:01<00:00, 2031.81it/s]\n",
      "2022-10-10 19:33:37,054 - s2and - INFO - preprocessed papers\n"
     ]
    }
   ],
   "source": [
    "# we're going to load the arnetminer dataset\n",
    "# and assume that you have it already downloaded to the `S2AND/data/` directory\n",
    "\n",
    "dataset_name = 'test'\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT_PATH, 'data')\n",
    "\n",
    "pddata = PDData(\n",
    "    papers=os.path.join(DATA_DIR, dataset_name + \"_papers.json\"),\n",
    "    name=dataset_name,\n",
    "    mode=\"train\",  # can also be 'inference' if just predicting\n",
    "    clusters=os.path.join(DATA_DIR, dataset_name + \"_clusters.json\"),\n",
    "    train_pairs=None,  # in case you have predefined splits for the pairwise models\n",
    "    val_pairs=None,\n",
    "    test_pairs=None,\n",
    "    train_pairs_size=100000,  # how many training pairs for the pairwise models?\n",
    "    val_pairs_size=10000,\n",
    "    test_pairs_size=10000,\n",
    "    n_jobs=n_jobs,\n",
    "    load_name_counts=False,\n",
    "    random_seed=random_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-10 19:33:38,284 - s2and - INFO - featurizing train\n",
      "2022-10-10 19:33:38,286 - s2and - INFO - Creating 2107 pieces of work\n",
      "2022-10-10 19:33:38,290 - s2and - INFO - Created pieces of work\n",
      "2022-10-10 19:33:38,291 - s2and - INFO - Cached changed, doing 2107 work in parallel\n",
      "2022-10-10 19:33:38,576 - s2and - INFO - Work completed\n",
      "2022-10-10 19:33:38,578 - s2and - INFO - Making numpy arrays for features and labels\n",
      "2022-10-10 19:33:38,580 - s2and - INFO - Numpy arrays made\n",
      "2022-10-10 19:33:38,583 - s2and - INFO - featurized train, featurizing val\n",
      "2022-10-10 19:33:38,584 - s2and - INFO - Creating 418 pieces of work\n",
      "2022-10-10 19:33:38,586 - s2and - INFO - Created pieces of work\n",
      "2022-10-10 19:33:38,587 - s2and - INFO - Cached changed, doing 418 work in parallel\n",
      "2022-10-10 19:33:38,711 - s2and - INFO - Work completed\n",
      "2022-10-10 19:33:38,714 - s2and - INFO - Making numpy arrays for features and labels\n",
      "2022-10-10 19:33:38,715 - s2and - INFO - Numpy arrays made\n",
      "2022-10-10 19:33:38,716 - s2and - INFO - featurized val, featurizing test\n",
      "2022-10-10 19:33:38,717 - s2and - INFO - Creating 251 pieces of work\n",
      "2022-10-10 19:33:38,719 - s2and - INFO - Created pieces of work\n",
      "2022-10-10 19:33:38,721 - s2and - INFO - Cached changed, doing 251 work in parallel\n",
      "2022-10-10 19:33:38,850 - s2and - INFO - Work completed\n",
      "2022-10-10 19:33:38,853 - s2and - INFO - Making numpy arrays for features and labels\n",
      "2022-10-10 19:33:38,855 - s2and - INFO - Numpy arrays made\n",
      "2022-10-10 19:33:38,857 - s2and - INFO - featurized test\n"
     ]
    }
   ],
   "source": [
    "# to train the pairwise model, we define which feature categories to use\n",
    "# here it is all of them\n",
    "features_to_use = [\n",
    "    \"author_similarity\",\n",
    "    \"venue_similarity\",\n",
    "    \"year_diff\",\n",
    "    \"title_similarity\",\n",
    "    \"abstract_similarity\",\n",
    "]\n",
    "\n",
    "# we store all the information about the features in this convenient wrapper\n",
    "featurization_info = FeaturizationInfo(features_to_use=features_to_use, featurizer_version=FEATURIZER_VERSION)\n",
    "\n",
    "# now we can actually go and get the pairwise training, val and test data\n",
    "train, val, test = featurize(pddata, featurization_info, n_jobs=4, use_cache=False, chunk_size=DEFAULT_CHUNK_SIZE, nan_value=np.nan)  # type: ignore\n",
    "X_train, y_train, nameless_X_train = train\n",
    "X_val, y_val, nameless_X_val = val\n",
    "X_test, y_test, nameless_X_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:05<00:00,  4.32trial/s, best loss: -0.9976514560972196]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<hyperopt.base.Trials at 0x7fcceda7add8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we define and fit the pairwise modelers\n",
    "pairwise_modeler = PairwiseModeler(\n",
    "    n_iter=25,  # number of hyperparameter search iterations\n",
    "    estimator=None,  # this will use the default LightGBM classifier\n",
    "    search_space=None,  # this will use the default LightGBM search space\n",
    "    monotone_constraints=featurization_info.lightgbm_monotone_constraints,  # we use monotonicity constraints to make the model more sensible\n",
    "    random_state=random_seed,\n",
    ")\n",
    "pairwise_modeler.fit(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-10 19:34:14,067 - s2and - INFO - Fitting clusterer\n",
      "2022-10-10 19:34:14,164 - s2and - INFO - Making 10 distance matrices\n",
      "2022-10-10 19:34:14,166 - s2and - INFO - Initializing pairwise_probas\n",
      "2022-10-10 19:34:14,167 - s2and - INFO - Pairwise probas initialized, starting making all pairs\n",
      "2022-10-10 19:34:14,168 - s2and - INFO - Featurizing batch 0/1\n",
      "2022-10-10 19:34:14,169 - s2and - INFO - Getting constraints\n",
      "2022-10-10 19:34:14,232 - s2and - INFO - Creating 16719 pieces of work\n",
      "2022-10-10 19:34:14,255 - s2and - INFO - Created pieces of work\n",
      "2022-10-10 19:34:14,256 - s2and - INFO - Cached changed, doing 16719 work in parallel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 770 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Doing work: 100%|██████████| 16719/16719 [00:00<00:00, 27796.19it/s]\n",
      "2022-10-10 19:34:15,048 - s2and - INFO - Work completed\n",
      "2022-10-10 19:34:15,049 - s2and - INFO - Making numpy arrays for features and labels\n",
      "2022-10-10 19:34:15,055 - s2and - INFO - Numpy arrays made\n",
      "2022-10-10 19:34:15,059 - s2and - INFO - Making predict flags\n",
      "2022-10-10 19:34:15,062 - s2and - INFO - Pairwise classification\n",
      "2022-10-10 19:34:15,106 - s2and - INFO - Starting to make matrices\n",
      "Writing matrices: 100%|██████████| 16719/16719 [00:00<00:00, 680953.28it/s]\n",
      "2022-10-10 19:34:15,134 - s2and - INFO - 10 distance matrices made\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 117.64trial/s, best loss: -0.769]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-10 19:34:15,358 - s2and - INFO - Clusterer fit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'eps': 0.6762519627824752}\n"
     ]
    }
   ],
   "source": [
    "# now we can fit the clusterer itself\n",
    "clusterer = Clusterer(\n",
    "    featurization_info,\n",
    "    pairwise_modeler.classifier,  # the actual pairwise classifier\n",
    "    cluster_model=FastCluster(linkage='average'),  # average linkage agglomerative clustering\n",
    "    search_space={\"eps\": hp.uniform(\"choice\", 0, 1)},  # the hyperparemetrs for the clustering algorithm\n",
    "    n_jobs=n_jobs,\n",
    "    use_cache=False,\n",
    "    random_state=random_seed,\n",
    ")\n",
    "clusterer.fit(pddata)\n",
    "print(clusterer.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C extension was not built during install!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LightGBM binary classifier with TreeExplainer shap values output has changed to a list of ndarray\n",
      "All-NaN slice encountered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUROC': 0.994, 'Average Precision': 0.993, 'F1': 0.983, 'Precision': 0.985, 'Recall': 0.982}\n"
     ]
    }
   ],
   "source": [
    "# but how good are our models? \n",
    "# first, let's look at the quality of the pairwise evaluation\n",
    "pairwise_metrics = pairwise_eval(\n",
    "    X_test,\n",
    "    y_test,\n",
    "    pairwise_modeler,\n",
    "    os.path.join(PROJECT_ROOT_PATH, \"data\", \"tutorial_figures\"),  # where to put the figures\n",
    "    \"tutorial_figures\",  # what to call the figures\n",
    "    featurization_info.get_feature_names(),\n",
    "    skip_shap=False,  # if your model isn't a tree-based model, you should put True here and it will not make SHAP figures\n",
    ")\n",
    "print(pairwise_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-10 19:33:55,211 - s2and - INFO - Making 11 distance matrices\n",
      "2022-10-10 19:33:55,212 - s2and - INFO - Initializing pairwise_probas\n",
      "2022-10-10 19:33:55,213 - s2and - INFO - Pairwise probas initialized, starting making all pairs\n",
      "2022-10-10 19:33:55,214 - s2and - INFO - Featurizing batch 0/1\n",
      "2022-10-10 19:33:55,214 - s2and - INFO - Getting constraints\n",
      "2022-10-10 19:33:55,235 - s2and - INFO - Creating 6717 pieces of work\n",
      "2022-10-10 19:33:55,245 - s2and - INFO - Created pieces of work\n",
      "2022-10-10 19:33:55,246 - s2and - INFO - Cached changed, doing 6717 work in parallel\n",
      "2022-10-10 19:33:55,637 - s2and - INFO - Work completed\n",
      "2022-10-10 19:33:55,639 - s2and - INFO - Making numpy arrays for features and labels\n",
      "2022-10-10 19:33:55,641 - s2and - INFO - Numpy arrays made\n",
      "2022-10-10 19:33:55,643 - s2and - INFO - Making predict flags\n",
      "2022-10-10 19:33:55,646 - s2and - INFO - Pairwise classification\n",
      "2022-10-10 19:33:55,668 - s2and - INFO - Starting to make matrices\n",
      "Writing matrices: 100%|██████████| 6717/6717 [00:00<00:00, 1107434.75it/s]\n",
      "2022-10-10 19:33:55,677 - s2and - INFO - 11 distance matrices made\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B3 (P, R, F1)': (0.639, 0.981, 0.774), 'Pred bigger ratio (mean, count)': (1.64, 78), 'True bigger ratio (mean, count)': (2.0, 1)}\n"
     ]
    }
   ],
   "source": [
    "# we can do the same thing for the clustering performance\n",
    "cluster_metrics, b3_metrics_per_signature = cluster_eval(\n",
    "    pddata,\n",
    "    clusterer,\n",
    "    split=\"test\",  # which part of the data to evaluate on, can also be 'val'\n",
    "    use_s2_clusters=False,  # set to true if you want to see how the old S2 system does\n",
    ")\n",
    "print(cluster_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('s2and')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "4a324344a38a2b71ef39af09a7448bcf45e5c27cf7a633e909c54afe4cf2573d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
