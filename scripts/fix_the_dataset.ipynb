{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from os.path import join\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "from s2and.data import PDData\n",
    "from s2and.consts import PROJECT_ROOT_PATH\n",
    "from pys2.pys2 import _evaluate_redshift_query, _load_dataframe_to_redshift\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(context='talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "with open('/net/nfs2.s2-research/shaurya/projects/s2pac/final_formatting/data/papers.v2.json', 'r') as f:\n",
    "    papers = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# buckets = pd.read_csv('/net/nfs2.s2-research/shaurya/projects/s2pac/clustering_data_gathering/prod_buckets.800M.v4.csv')\n",
    "# buckets_map = {}\n",
    "# for row in buckets.itertuples():\n",
    "#     for paper_id in eval(row.prod_bucket):\n",
    "#         buckets_map[str(paper_id)] = row.key_35_w_colon\n",
    "#         buckets_map[int(paper_id)] = row.key_35_w_colon\n",
    "        \n",
    "# buckets_map_sub = {}\n",
    "# for paper in papers.keys():\n",
    "#     buckets_map_sub[paper] = buckets_map[paper]\n",
    "    \n",
    "# # save buckets_map_sub so we don't have to do this again\n",
    "# with open('/net/nfs2.s2-research/shaurya/projects/s2pac/clustering_data_gathering/prod_buckets.v2.json', 'w') as f:\n",
    "#     json.dump(buckets_map_sub, f)\n",
    "\n",
    "with open('/net/nfs2.s2-research/shaurya/projects/s2pac/clustering_data_gathering/prod_buckets.v2.json', 'r') as f:\n",
    "    buckets_map = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2911289 15520\n"
     ]
    }
   ],
   "source": [
    "# step 0: many papers are in the wrong block. delete them\n",
    "papers_to_delete = []\n",
    "for paper_id, paper in papers.items():\n",
    "    if paper['block'] != buckets_map[str(paper_id)]:\n",
    "        papers_to_delete.append(paper_id)\n",
    "print(len(papers), len(papers_to_delete))\n",
    "\n",
    "for paper_id in papers_to_delete:\n",
    "    del papers[paper_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23729 20\n"
     ]
    }
   ],
   "source": [
    "# step 1: we need to make sure that every paper is in a cluster that is inside the *correct* block\n",
    "# if not, we delete the paper from the cluster in the incorrect block\n",
    "with open('/net/nfs2.s2-research/shaurya/projects/s2pac/final_formatting/data/clusters.v2.json', 'r') as f:\n",
    "    clusters = json.load(f)\n",
    "\n",
    "times_trimmed = 0\n",
    "cluster_ids_to_delete = set()\n",
    "for cluster_id in clusters.keys():\n",
    "    papers_loop = clusters[cluster_id]['paper_ids']\n",
    "    this_block = cluster_id.split('_')[0]\n",
    "    papers_for_this_block = [\n",
    "        p for p in papers_loop \n",
    "        if str(p) in papers \n",
    "        and papers[str(p)]['block'] == this_block\n",
    "    ]  \n",
    "    if len(papers_for_this_block) > 0:\n",
    "        clusters[cluster_id]['paper_ids'] = papers_for_this_block\n",
    "    else:\n",
    "        cluster_ids_to_delete.add(cluster_id)\n",
    "    if len(papers_for_this_block) < len(papers_loop):\n",
    "        times_trimmed += 1\n",
    "print(times_trimmed, len(cluster_ids_to_delete))\n",
    "\n",
    "for key in cluster_ids_to_delete:\n",
    "    del clusters[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 2: a bunch of papers are in clusters AND in orphans. we need to remove them from the orphans\n",
    "papers_to_clusters = defaultdict(set)\n",
    "for cluster_id, cluster in clusters.items():\n",
    "    for paper_id in cluster['paper_ids']:\n",
    "        papers_to_clusters[paper_id].add(cluster_id)\n",
    "        \n",
    "total_removed = 0\n",
    "for paper_id, cluster_ids in papers_to_clusters.items():\n",
    "    if len(cluster_ids) > 1:\n",
    "        orphan_clusters = [c for c in cluster_ids if c.endswith('_orphans')]\n",
    "        orphan_cluster_blocks = [c.split('_')[0] for c in orphan_clusters]\n",
    "        nonorphan_clusters = [c for c in cluster_ids if not c.endswith('_orphans')]\n",
    "        nonorphan_clusters_blocks = [c.split('_')[0] for c in nonorphan_clusters]\n",
    "        # if the paper is in an orphan cluster, and there is a non-orphan cluster in the same block, remove it from the orphan cluster\n",
    "        to_remove = [c for b, c in zip(orphan_cluster_blocks, orphan_clusters) if b in nonorphan_clusters_blocks]\n",
    "        total_removed += len(to_remove)\n",
    "        for cluster_id in to_remove:\n",
    "            try: # i really don't know the types of these keys\n",
    "                clusters[cluster_id]['paper_ids'].remove(str(paper_id))\n",
    "            except:\n",
    "                clusters[cluster_id]['paper_ids'].remove(int(paper_id))\n",
    "total_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 3a: a bunch of papers are in multiple clusters in the SAME block. probably easiest to just remove these blocks entirely\n",
    "papers_to_clusters = defaultdict(set)\n",
    "for cluster_id, cluster in clusters.items():\n",
    "    for paper_id in cluster['paper_ids']:\n",
    "        papers_to_clusters[paper_id].add(cluster_id)\n",
    "        \n",
    "papers_in_multiple_clusters = set([int(p) for p, c in papers_to_clusters.items() if len(c) > 1] + [str(p) for p, c in papers_to_clusters.items() if len(c) > 1])\n",
    "len(papers_in_multiple_clusters)\n",
    "blocks_to_remove = set([papers[str(p)]['block'] for p in papers_in_multiple_clusters])\n",
    "len(blocks_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 200\n"
     ]
    }
   ],
   "source": [
    "# step 3b need to find blocks with bad paper names and remove them\n",
    "from s2and.text import normalize_text\n",
    "\n",
    "bad_paper_names = {\n",
    "    'comment', \n",
    "    'comments',\n",
    "    'discussion',\n",
    "    'book',\n",
    "    'response', \n",
    "    'letter', \n",
    "    'reply', \n",
    "    'reply to', \n",
    "    're', \n",
    "    'withdrawn', \n",
    "    'note', \n",
    "    'notes', \n",
    "    'note on', \n",
    "    'notes on', \n",
    "    \"author correction\",\n",
    "    \"book review\",\n",
    "    \"book reviews\",\n",
    "    \"case report\",\n",
    "    \"characterization\",\n",
    "    \"cheminform abstract\",\n",
    "    \"commentary\",\n",
    "    \"copyright\",\n",
    "    \"correction to\",\n",
    "    \"correction\",\n",
    "    \"editorial board\",\n",
    "    \"editorial\",\n",
    "    \"erratum\",\n",
    "    \"feature article\",\n",
    "    \"in this issue\",\n",
    "    \"introduction\",\n",
    "    \"libguides\",\n",
    "    \"pii\",\n",
    "    \"republished\",\n",
    "    \"response to\",\n",
    "    \"retracted article\",\n",
    "    \"retracted\",\n",
    "    \"review\",\n",
    "    \"withdrawal\"\n",
    "}\n",
    "\n",
    "paper_ids_that_are_bad = set()\n",
    "blocks_that_are_bad = blocks_to_remove\n",
    "for k, v in papers.items():\n",
    "    if normalize_text(v['title']) in bad_paper_names:\n",
    "        paper_ids_that_are_bad.add(k)\n",
    "        blocks_that_are_bad.add(v['block'])\n",
    "\n",
    "print(len(paper_ids_that_are_bad), len(blocks_that_are_bad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some blocks now have no non-orphan clusters in them, so we can just drop them\n",
    "non_orphan_blocks = {k.split('_')[0] for k in clusters.keys() if not k.endswith('_orphans')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256483, 2891909)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep only the papers in the blocks we want to keep\n",
    "papers_sub = {k: v for k, v in papers.items() if v['block'] not in blocks_that_are_bad and v['block'] in non_orphan_blocks}\n",
    "\n",
    "# keep only clusters where all of the papers are in papers_sub\n",
    "clusters_sub = {k: v for k, v in clusters.items() if all([str(p) in papers_sub for p in v['paper_ids']])}\n",
    "\n",
    "# keep only papers that are in a cluster\n",
    "papers_in_clusters = set()\n",
    "for cluster_id in clusters_sub.keys():\n",
    "    papers_in_clusters.update(set([str(i) for i in clusters[cluster_id]['paper_ids']]))\n",
    "    \n",
    "assert papers_in_clusters == set(papers_sub.keys())\n",
    "len(clusters_sub), len(papers_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a table of all of these keys\n",
    "sourced_paper_ids = pd.DataFrame({'sourced_paper_id': list(papers_sub.keys())})\n",
    "\n",
    "_load_dataframe_to_redshift(\n",
    "    sourced_paper_ids,\n",
    "    \"public.temp_clustering_cluster_source_id\",\n",
    "    create_table=True,\n",
    "    write_privileges=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2766227\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"select ps.sourced_paper_id, ps.source, ps.source_id, pdf_hash from content.paper_sources ps join public.temp_clustering_cluster_source_id t on ps.sourced_paper_id = t.sourced_paper_id\"\"\"\n",
    "df_source_ids = _evaluate_redshift_query(query)\n",
    "print(len(df_source_ids))\n",
    "for row in df_source_ids.itertuples():\n",
    "    paper = papers_sub[str(row.sourced_paper_id)]\n",
    "    assert paper['source'] == row.source\n",
    "    if row.source_id is not None:\n",
    "        paper['source_id'] = row.source_id\n",
    "    if row.pdf_hash is not None:\n",
    "        paper['pdf_hash'] = row.pdf_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('authors', 2891909),\n",
       " ('abstract', 2891909),\n",
       " ('references', 2891909),\n",
       " ('paper_id', 2891909),\n",
       " ('source', 2891909),\n",
       " ('title', 2891909),\n",
       " ('block', 2891909),\n",
       " ('corpus_paper_id', 2891909),\n",
       " ('year', 2784230),\n",
       " ('source_id', 2766227),\n",
       " ('journal_name', 2122651),\n",
       " ('doi', 1802724),\n",
       " ('venue', 1191272),\n",
       " ('fieldsofstudy', 1066676),\n",
       " ('pdf_hash', 995240),\n",
       " ('publicationdate', 977640),\n",
       " ('publicationtypes', 394136),\n",
       " ('pmid', 332049),\n",
       " ('openaccesslocation', 195667),\n",
       " ('publisher', 185063),\n",
       " ('updatedate', 1801),\n",
       " ('bibliography', 80)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count all of the fields in the papers\n",
    "fields = Counter()\n",
    "for v in papers_sub.values():\n",
    "    fields.update(v.keys())\n",
    "    \n",
    "fields.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save next version\n",
    "with open('/net/nfs2.s2-research/shaurya/projects/s2pac/final_formatting/data/papers.v3.json', 'w') as f:\n",
    "    json.dump(papers_sub, f)\n",
    "\n",
    "with open('/net/nfs2.s2-research/shaurya/projects/s2pac/final_formatting/data/clusters.v3.json', 'w') as f:\n",
    "    json.dump(clusters_sub, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now downsample the blocks that have 100% accuracy\n",
    "# load the data\n",
    "with open('/net/nfs2.s2-research/shaurya/projects/s2pac/final_formatting/data/papers.v3.json', 'r') as f:\n",
    "    papers = json.load(f)\n",
    "\n",
    "with open('/net/nfs2.s2-research/shaurya/projects/s2pac/final_formatting/data/clusters.v3.json', 'r') as f:\n",
    "    clusters = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 9978 1410 67217 13098\n",
      "33608\n",
      "46706\n",
      "51023\n",
      "49897\n"
     ]
    }
   ],
   "source": [
    "# now let's drop a subset of the \"easy\" blocks\n",
    "df = pd.read_csv(join(PROJECT_ROOT_PATH, \"data\", \"block_removal_candidates.csv\"))\n",
    "blocks_to_keep = set(df['block'][df.keep_count > 0].values)\n",
    "blocks_to_drop = set(df['block'][df.remove_count > 0].values)\n",
    "easy_blocks = df['block'][df.accuracy == 1.0].values\n",
    "not_easy_blocks = df['block'][df.accuracy != 1.0].values\n",
    "print(len(blocks_to_keep.intersection(blocks_to_drop)), len(blocks_to_keep), len(blocks_to_drop), len(easy_blocks), len(not_easy_blocks))\n",
    "# choose a random half of the easy blocks and keep all the hard blocks\n",
    "keep_blocks = set(np.random.choice(easy_blocks, size=len(easy_blocks)//2, replace=False))\n",
    "print(len(keep_blocks))\n",
    "keep_blocks.update(not_easy_blocks)\n",
    "print(len(keep_blocks))\n",
    "keep_blocks.update(blocks_to_keep)\n",
    "print(len(keep_blocks))\n",
    "keep_blocks -= blocks_to_drop\n",
    "print(len(keep_blocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the papers in the blocks we want to keep\n",
    "papers_sub = {k: v for k, v in papers.items() if v['block'] in keep_blocks}\n",
    "\n",
    "# keep only clusters where all of the papers are papers2\n",
    "clusters_sub = {k: v for k, v in clusters.items() if all([str(p) in papers_sub for p in v['paper_ids']])}\n",
    "\n",
    "# keep only papers that are in a cluster\n",
    "papers_in_clusters = set()\n",
    "for cluster_id in clusters_sub.keys():\n",
    "    papers_in_clusters.update(set([str(i) for i in clusters[cluster_id]['paper_ids']]))\n",
    "    \n",
    "assert papers_in_clusters == set(papers_sub.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save next version\n",
    "with open('/net/nfs2.s2-research/shaurya/projects/s2pac/final_formatting/data/papers.v3_hard.json', 'w') as f:\n",
    "    json.dump(papers_sub, f)\n",
    "\n",
    "with open('/net/nfs2.s2-research/shaurya/projects/s2pac/final_formatting/data/clusters.v3_hard.json', 'w') as f:\n",
    "    json.dump(clusters_sub, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('s2and')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a324344a38a2b71ef39af09a7448bcf45e5c27cf7a633e909c54afe4cf2573d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
